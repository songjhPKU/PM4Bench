<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PM4Bench</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- 此处开始网站的正式部分 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img id="teaser" src="./static/images/icon.png" alt="Teaser image" style="height: 1em; vertical-align: middle; display: inline;">
            <span class="gradient-text">PM<sup>4</sup>Bench</span>: A <u>P</u>arallel <u>M</u>ultilingual <u>M</u>ulti-<u>M</u>odal <u>M</u>ulti-task Benchmark for Large Vision Language Model
          </h1>
          
          <div class="is-size-5 publication-authors">
            <div class="author-raw">
              <span class="author-block">
                <a href="https://keunhong.com">Junyuan Gao</a><sup>2*</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Jiahe Song</a><sup>3*</sup>,</span>
              <span class="author-block">
                <a href="https://jonbarron.info">Jiang Wu</a><sup>1*†</sup>,
              </span>
            </div>
            <div class="author-raw">
              <span class="author-block">
                <a href="https://keunhong.com">Runchuan Zhu</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Guanlin Shen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Shasha Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Xingjian Wei</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Haote Yang</a><sup>1</sup>,</span>
            </div>
            <div class="author-raw">
              <span class="author-block">
                <a href="https://keunhong.com">Songyang Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Weijia Li</a><sup>4,1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Bin Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Dahua Lin</a><sup>1,5</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Lijun Wu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiahe-song.webflow.io">Conghui He</a><sup>1‡</sup>,</span>  
            </div>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory,</span>
            <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>3</sup>Peking University,</span>
            <span class="author-block"><sup>4</sup>Sun Yat-Sen University,</span>
            <span class="author-block"><sup>5</sup>Chinese University of Hong Kong</span>
          </div>

          <p class="is-size-7 has-text-grey mt-2">
            <sup>*</sup> Equal contribution &nbsp;&nbsp; <sup>†</sup> Project lead &nbsp;&nbsp; <sup>‡</sup> Correspondence: <a href="mailto:heconghui@pjlab.org.cn"> heconghui@pjlab.org.cn
          </p>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.18484"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/opendatalab/PM4Bench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/songjhPKU/PM4Bench/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      🤗
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/intro.png" alt="Teaser image" style="width: 100%;"></img>
      <h2 class="subtitle has-text-centered">
        <span>
          Overview of <span class="gradient-text">PM<sup>4</sup>Bench</span>, which includes parallel corpora in 10 languages and features two settings:
          <code>traditional</code> and <code>vision</code>, with four tasks: MDUR, MIQA, MMJB, and MSOCR.
          Based on <span class="gradient-text">PM<sup>4</sup>Bench</span>, we comprehensively evaluate the usefulness and safety of LVLMs,
          delving into the relationship between their underlying OCR capabilities and higher-level abilities.
        </span>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing multilingual benchmarks for Large Vision Language Models (LVLMs) suffer from limitations including language-specific content biases, 
            disjointed multimodal input formats, and a lack of safety evaluation. To address these gaps, we propose <strong><span class="gradient-text">PM<sup>4</sup>Bench</span>, 
            the first Parallel Multilingual Multi-Modal Multi-task Benchmark for LVLMs</strong>. <span class="gradient-text">PM<sup>4</sup>Bench</span> features a parallel corpus design across 10 languages, 
            enabling fair and accurate cross-lingual comparisons. It includes the <code>vision</code> setting where text and queries are embedded in images, 
            requiring LVLMs to simultaneously <q>see</q>, <q>read</q>, and <q>think</q>, aligning with real-world applications. 
            Additionally, <span class="gradient-text">PM<sup>4</sup>Bench</span> incorporates safety evaluations, addressing critical oversight in existing multilingual benchmarks. 
            Using <span class="gradient-text">PM<sup>4</sup>Bench</span>, we evaluate 11 mainstream LVLMs, revealing significant cross-linguistic performance disparities, particularly in <code>vision</code> settings, 
            and identifying OCR capability as a key determinant of these imbalances. 
            We released <span class="gradient-text">PM<sup>4</sup>Bench</span> at <a href="https://github.com/opendatalab/PM4Bench">https://github.com/opendatalab/PM4Bench</a>.
          </p>
          <p style="color: red;">
            Warning: This paper contains potentially offensive and harmful text.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Highlights. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths highlight-box">
        <h2 class="title is-3">🔥 Highlight</h2>
        <div class="content has-text-justified">
          <ul class="highlight-list">
            <li><strong>Parallel Text for Multi-Modal:</strong> We offer the first Parallel Multilingual Multi-Modal Multi-task Benchmark on 10 parallel corpus, enabling fair and in-depth multilingual evaluation and analysis.</li>
            <li><strong>Comprehensive Evaluation:</strong> We conduct extensive evaluations for 11 LVLMs, setting up a comprehensive foundation for comparative analysis.</li>
            <li><strong>Meticulous Analysis:</strong> We conduct further analysis that reveals greater imbalance in <code>vision</code> settings, and OCR capability has strong correlation to LVLM's performance, providing guidance for future advance.</li>
          </ul>
        </div>
      </div>
    </div>
    
    <!--/ Highlights. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</span></h2>
        <div class="content has-text-justified">
          <p>
            Comprehensive evaluation of LVLMs in multilingual scenarios is crucial for identifying shortcomings and guiding further optimization. 
            However, most existing benchmarks have certain limitations: <strong>(1) Some rely on language-specific corpora,</strong> coupling linguistic ability with cultural knowledge, 
            making it difficult to discern whether performance gaps arise from cultural knowledge deficiencies or fundamental linguistic capabilities; 
            <strong>(2) Text and images are processed separately,</strong>  unlike how humans naturally interact with multi-modal information in the real world; and 
            <strong>(3) Safety evaluation is neglected,</strong> posing risks for responsible deployment.
          </p>
          <p>To address these gaps, we propose <strong><span class="gradient-text">PM<sup>4</sup>Bench</span>, the first Parallel Multilingual Multi-Modal Multi-task Benchmark for LVLMs</strong>. 
            <span class="gradient-text">PM<sup>4</sup>Bench</span> includes 10 languages and uses parallel corpora focused on world knowledge, decoupling performance from cultural contexts. 
            It includes the <code>vision</code> setting where text and queries are embedded in images, which align with real-world application scenarios such as multi-modal agents, 
            free-form web interaction, and perception and self-learning of embodied AI robots. Additionally, <span class="gradient-text">PM<sup>4</sup>Bench</span> evaluates LVLM safety in multilingual and multi-modal contexts, 
            filling a critical gap. Detailed comparison between <span class="gradient-text">PM<sup>4</sup>Bench</span> and other benchmarks are listed in <a href="#benchmark-comparison"> Benchmark Comparison </a>.</p>
        
          <p>Using <span class="gradient-text">PM<sup>4</sup>Bench</span>, we evaluated 11 LVLMs, including leading open-sourced LVLMs, commercial APIs, light-weight LVLMs, and recent reasoning LVLMs, 
            revealing significant cross-linguistic performance disparities, particularly in <a href="#radar_overall"><code>vision</code> settings</a>. We found that increasing model size does not mitigate these imbalances, 
            with Optical Character Recognition (OCR) capability identified as the key factor.</p>
        </div>

        <!-- Two pics, Benchmark + Results -->
        <section class="image-gallery">
          <div class="image-row">
            <div class="image-item" id="benchmark-comparison">
              <img src="./static/images/benchmark_comparison.png" alt="Image 1">
              <figcaption>Benchmark Comparison.</figcaption>
            </div>
            
            <div class="image-item" id="radar_overall">
              <img src="./static/images/radar_overall.png" alt="Image 2">
              <figcaption>Radar chart of overall vision setting performance on MDUR, MIQA, MMJB and MSOCR.</figcaption>
            </div>
          </div>
        </section>
        <!-- /Two pics, Benchmark + Results -->

      </div>
      
    </div>
    <!-- / Introduction -->
    <!-- Overview of PM4Bench -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview of <span class="gradient-text">PM<sup>4</sup>Bench</span></h2>
        <h3 class="title is-4">Design Principles</span></h3>
        <div class="content has-text-justified">
          <p>
            Our core motivation is to comprehensively evaluate the performance of LVLMs in both usefulness and safety within multilingual &amp; 
            multi-modal scenarios. We aim to align more closely with real-world user applications, and assess LVLMs' cross-lingual performance disparities faithfully 
            and systematically. Furthermore, we aim to accurately analyze and identify the underlying issues and shortcomings of current LVLMs, providing clear guidance 
            for model optimization.
          </p>
          
          <p>To achieve this, we propose the following design principles:</p>
          
          <ul class="design-principles">
            <li><strong>Targeted Language Selection:</strong> The selected languages should cover diverse language families, varying different writing scripts.</li>
            <li><strong>Parallel Corpus:</strong> The content across languages must be semantically identical. This ensures that language-specific and culturally related knowledge is decoupled from the evaluation tasks, allowing us to remain focused on assessing fundamental language capabilities.</li>
            <li><strong><code>Vision</code> Setting:</strong> To simulate real-world applications and human perception, text and queries are "printed" onto images in <code>vision</code> setting.</li>
            <li><strong>Task Diversity:</strong> The benchmark should encompass a wide range of tasks, including perception, knowledge recall and reasoning, generation, and safety.</li>
          </ul>   
        </div>
        <h3 class="title is-4">Task Introduction</span></h3>
        <section class="image-gallery">
          <div class="image-row">
            <div class="image-item-small" id="basic+langs">
              <img src="./static/images/basic+langs.png" alt="Image 1">
              <figcaption>Above: basic statistics information of <span class="gradient-text">PM<sup>4</sup>Bench</span>.</figcaption>
              <figcaption>Below: language Information Table. --- indicates that GraphCom does not provide specific numerical values. 
                However, by comparing the number of characters, language families, and other aspects of the script systems, 
                we have identified the rankings of Czech and Vietnamese in the table. </figcaption>
            </div>
            
            <div class="image-item-small" id="trad_vs_vis">
              <img src="./static/images/trad_vs_vis.png" alt="Image 2">
              <figcaption><p>Comparison between <code>traditional</code> and <code>vision</code> setting. 
                Under <code>vision</code> setting, the input of LVLM is a single image containing all the information needed to fulfill the task.</p>
                Under <code>traditional</code> setting,  text content of questions and inserted images are separately given.</figcaption>
              </figcaption>
            </div>
          </div>
        </section>


        <!-- Four Datasets -->
        <div class="container is-max-desktop">
          <!-- MDUR -->
          <div class="column is-full-width dataset-box">
            <h2 class="title is-5"> 🔍 MDUR (Multi-Discipline Understanding and Reasoning)</h2>
            <div class="content has-text-justified">
              <p>
              MDUR aims to evaluate LVLM's multi-modal understanding, knowledge application and reasoning capability. 
              Thus, we chose <a href="https://mmmu-benchmark.github.io/">MMMU-pro</a> as our data source. MMMU-pro is a comprehensive dataset created to assess multi-modal models on college-level tasks 
              that demand specialized knowledge and critical reasoning. MMMU-pro has 1730 samples, each of which is an English multi-choice question with only one correct option.
              </p>
              <p>
              We translate the text of original English questions into the 9 other languages and generated the vision form images. 
              It is important to note that some of the inserted images in the MDUR task contain English characters, inherited from the original MMMU-pro samples.
               We believe that their presence has minimal impact on our <strong>"parallel"</strong> design principle.
              </p>
              <p>
              Finally, we obtain the MDUR dataset covering 10 languages, with 1730 questions for each language. 
              With MDUR task, we are able to extensively evaluate LVLM's capability to handle complicated knowledge understanding, 
              reasoning, and application under multilingual senarios. Examples of MDUR samples can be found below (Left is <code>traditional</code> and right is <code>vision</code> setting).
              </p>
            </div>
            <section class="image-gallery">
              <div class="image-row">
                <div class="image-item-dataset" id="MDUR_trad">
                  <img src="./static/images/MDUR_trad.png" alt="Image 1">
                </div>
                
                <div class="image-item-dataset id="MDUR_vision">
                  <img src="./static/images/MDUR_vision.png" alt="Image 2">
                </div>
              </div>
            </section>
          </div>
          <!-- / MDUR -->
          <!-- MIQA -->
          <div class="column is-full-width dataset-box">
            <h2 class="title is-5"> 💬 MIQA (Multi-Image Question Answering)</h2>
            <div class="content has-text-justified">
              <p>
              MIQA focuses on open-end question answering capabilities in multi-image input scenarios. We used <a href="https://liuziyu77.github.io/MMDU/">MMDU</a>, 
              a multi-turn multi-image dialog understanding benchmark containing 1.6K+ rounds of QA as our source of data. 
              We sampled 109 QA pairs from MMDU, where we prioritized choosing questions that included more image inputs. 
              These questions and corresponding reference answers are then translated into the 9 languages. 
              Similar to MDUR task, we also provide both <code>traditional</code> and <code>vision</code> input setting for MIQA task.
              </p>
              <p>
              We translate the text of original English questions into the 9 other languages and generated the vision form images. 
              It is important to note that some of the inserted images in the MDUR task contain English characters, inherited from the original MMMU-pro samples.
               We believe that their presence has minimal impact on our <strong>"parallel"</strong> design principle.
              </p>
              <p>
              Finally, we obtain the MDUR dataset covering 10 languages, with 1730 questions for each language. 
              With MDUR task, we are able to extensively evaluate LVLM's capability to handle complicated knowledge understanding, 
              reasoning, and application under multilingual senarios. Examples of MDUR samples can be found below (Left is <code>traditional</code> and right is <code>vision</code> setting).
              </p>
            </div>
            <section class="image-gallery">
              <div class="image-row">
                <div class="image-item-dataset" id="MDUR_trad">
                  <img src="./static/images/MDUR_trad.png" alt="Image 1">
                </div>
                
                <div class="image-item-dataset id="MDUR_vision">
                  <img src="./static/images/MDUR_vision.png" alt="Image 2">
                </div>
              </div>
            </section>
          </div>
          <!-- / MIQA -->
        </div>
      </div>
    </div>
    <!-- / Overview of PM4Bench -->
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
